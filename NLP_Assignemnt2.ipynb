{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98j4VVDwL7A_",
        "outputId": "a3434ed2-b831-4bf9-e818-1154e68e8f5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langid\n",
            "  Downloading langid-1.1.6.tar.gz (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from langid) (1.23.5)\n",
            "Building wheels for collected packages: langid\n",
            "  Building wheel for langid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langid: filename=langid-1.1.6-py3-none-any.whl size=1941172 sha256=18bb911ea262c580f994864cdb7670ca0c150cdea9c7713141cd50acd87e39c0\n",
            "  Stored in directory: /root/.cache/pip/wheels/23/c8/c6/eed80894918490a175677414d40bd7c851413bbe03d4856c3c\n",
            "Successfully built langid\n",
            "Installing collected packages: langid\n",
            "Successfully installed langid-1.1.6\n"
          ]
        }
      ],
      "source": [
        "!pip install langid"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import langid\n",
        "\n",
        "with open('/content/New Delhi.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "#perform language detection\n",
        "lang, confidence = langid.classify(text)\n",
        "\n",
        "#output the detected language and confidence score\n",
        "print(\"Detected language:\", lang)\n",
        "print(\"Confidence score:\", confidence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moRzM5uyMPPr",
        "outputId": "4a9f17a8-7842-407b-e279-485b35f7ee08"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected language: en\n",
            "Confidence score: -3445.3659179210663\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "nltk.download('maxnet_ne_chunker')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "with open('/content/New Delhi.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "#tokenize the text\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "#perform pos tag\n",
        "pos_tags = pos_tag(tokens)\n",
        "\n",
        "#perform NER\n",
        "ner_result = ne_chunk(pos_tags)\n",
        "\n",
        "#print the named entities\n",
        "for entity in ner_result:\n",
        "  if isinstance(entity, nltk.Tree):\n",
        "    entity_name = \" \".join([word for word, tag in entity.leaves()])\n",
        "    entity_type = entity.label()\n",
        "    print(f\"Entity: {entity_name}, Type: {entity_type}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ndn8PFpjM56W",
        "outputId": "3b69448c-c4b5-430e-a442-045202cfad44"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Error loading maxnet_ne_chunker: Package\n",
            "[nltk_data]     'maxnet_ne_chunker' not found in index\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entity: New, Type: GPE\n",
            "Entity: Delhi CNN, Type: PERSON\n",
            "Entity: India, Type: GPE\n",
            "Entity: Narendra Modi, Type: PERSON\n",
            "Entity: Indian, Type: GPE\n",
            "Entity: Modi, Type: PERSON\n",
            "Entity: Lakshadweep, Type: GPE\n",
            "Entity: Indian, Type: GPE\n",
            "Entity: Laccadive Sea, Type: ORGANIZATION\n",
            "Entity: Kerala, Type: GPE\n",
            "Entity: Modi, Type: PERSON\n",
            "Entity: India, Type: GPE\n",
            "Entity: Maldives, Type: ORGANIZATION\n",
            "Entity: Maldives, Type: ORGANIZATION\n",
            "Entity: Modi, Type: PERSON\n",
            "Entity: Israel, Type: GPE\n",
            "Entity: Reuters, Type: ORGANIZATION\n",
            "Entity: Maldives, Type: GPE\n",
            "Entity: Ministry, Type: ORGANIZATION\n",
            "Entity: Youth, Type: GPE\n",
            "Entity: Information, Type: ORGANIZATION\n",
            "Entity: Arts, Type: GPE\n",
            "Entity: Maldives, Type: GPE\n",
            "Entity: Reuters, Type: ORGANIZATION\n",
            "Entity: Maldives, Type: GPE\n",
            "Entity: Maldives, Type: ORGANIZATION\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "#load the spacy english model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "with open('/content/New Delhi.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "#process the text with spacy\n",
        "doc = nlp(text)\n",
        "\n",
        "#print the named entities\n",
        "for ent in doc.ents:\n",
        "  print(f\"Entity: {ent.text}, Type: {ent.label_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7jtLqwaNAUR",
        "outputId": "9c45621a-1fc3-438b-8a07-78201f874fbc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entity: New Delhi, Type: GPE\n",
            "Entity: Maldives, Type: GPE\n",
            "Entity: three, Type: CARDINAL\n",
            "Entity: India, Type: GPE\n",
            "Entity: Narendra Modi, Type: PERSON\n",
            "Entity: Indian, Type: NORP\n",
            "Entity: Modi, Type: GPE\n",
            "Entity: Lakshadweep, Type: GPE\n",
            "Entity: Indian, Type: NORP\n",
            "Entity: the Laccadive Sea, Type: LOC\n",
            "Entity: Kerala, Type: GPE\n",
            "Entity: India, Type: GPE\n",
            "Entity: Maldives, Type: GPE\n",
            "Entity: Maldives, Type: GPE\n",
            "Entity: Three, Type: CARDINAL\n",
            "Entity: Maldives, Type: GPE\n",
            "Entity: Modi, Type: GPE\n",
            "Entity: Israel, Type: GPE\n",
            "Entity: Reuters, Type: ORG\n",
            "Entity: Maldives, Type: GPE\n",
            "Entity: three, Type: CARDINAL\n",
            "Entity: the Ministry of Youth Employment, Information and Arts - a senior, Type: ORG\n",
            "Entity: Maldives, Type: GPE\n",
            "Entity: Reuters, Type: ORG\n",
            "Entity: Sunday, Type: DATE\n",
            "Entity: Maldives, Type: GPE\n",
            "Entity: Maldives, Type: ORG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "with open('/content/New Delhi.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# create sentimentintensityanalyzer object\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "# get the sentiment scores for the text\n",
        "sentiment_scores = sid.polarity_scores(text)\n",
        "\n",
        "print(\"Sentiment Scores:\", sentiment_scores)\n",
        "\n",
        "if sentiment_scores['compound'] >= 0.05:\n",
        "  sentiment_label = 'Positive'\n",
        "elif sentiment_scores['compound'] <= -0.05:\n",
        "  sentiment_label = 'Negative'\n",
        "else:\n",
        "  sentiment_label = 'Neutral'\n",
        "\n",
        "print(\"Overall Sentiment:\", sentiment_label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4Dr8VD0NJWk",
        "outputId": "b164b609-4306-4c34-e800-a448bbc1fa91"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment Scores: {'neg': 0.046, 'neu': 0.838, 'pos': 0.117, 'compound': 0.974}\n",
            "Overall Sentiment: Positive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "with open('/content/New Delhi.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "#create tfidf vectorizer\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "x = vectorizer.fit_transform([text])\n",
        "\n",
        "#get feature name and corresponding tfidf scores\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "tfidf_scores = x.toarray()[0]\n",
        "\n",
        "#combine feature names and scores and sort by scores\n",
        "key_phrases = [(feature, score) for feature, score in zip(feature_names, tfidf_scores)]\n",
        "key_phrases = sorted(key_phrases, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "#print the top key phrases\n",
        "print(\"Top key phrases:\")\n",
        "for phrase in key_phrases[:5]:\n",
        "  print(f\"{phrase}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWXZYyYaNLm-",
        "outputId": "cc56a8af-32b0-4df3-b277-4ddb7f3be054"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top key phrases:\n",
            "('maldives', 0.5275043787166296)\n",
            "('modi', 0.2637521893583148)\n",
            "('government', 0.1978141420187361)\n",
            "('officials', 0.1978141420187361)\n",
            "('comments', 0.1318760946791574)\n"
          ]
        }
      ]
    }
  ]
}